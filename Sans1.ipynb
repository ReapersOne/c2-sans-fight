{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d810585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import Dense\n",
    "from keras.api.optimizers import Adam\n",
    "from collections import deque\n",
    "from IPython.display import IFrame\n",
    "import pyautogui\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "from PIL import ImageGrab\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf3b9738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_player_y():\n",
    "    screenshot = pyautogui.screenshot()\n",
    "    img = cv2.cvtColor(np.array(screenshot), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Detect red heart (adjust HSV thresholds)\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    lower_red = np.array([0, 120, 70])\n",
    "    upper_red = np.array([10, 255, 255])\n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    \n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        return y + h // 2  # Middle of the heart\n",
    "    return 0.0  # Fallback  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1ae3aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bone_positions():\n",
    "    # Capture game region (adjust coordinates to Sans' attack area)\n",
    "    screenshot = pyautogui.screenshot(region=(300, 200, 400, 300))\n",
    "    img = cv2.cvtColor(np.array(screenshot), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Detect white bones\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, threshold = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(threshold, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Get bone centers and sort by danger (closest to player)\n",
    "    bones = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        bones.append((x + w//2, y + h//2))  # Store (x,y) center\n",
    "    \n",
    "    # Sort bones by Y-position (lower = closer to player)\n",
    "    bones.sort(key=lambda pos: pos[1], reverse=True)\n",
    "    return bones[:5]  # Return top 5 most threatening bones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15022cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0   # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state[np.newaxis, :])[0])\n",
    "            target_f = self.model.predict(state[np.newaxis, :])\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state[np.newaxis, :], target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "# You would need to modify the game to:\n",
    "# 1. Provide state observations (player position, bullet positions, etc.)\n",
    "# 2. Accept discrete actions (move left, right, up, down, etc.)\n",
    "# 3. Calculate rewards (survival time, damage dealt, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcdd75a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Launch Chrome and load the game\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://jcw87.github.io/c2-sans-fight/\")  # Hosted game URL\n",
    "time.sleep(2)  # Wait for game to load\n",
    "\n",
    "# Focus the game window\n",
    "driver.find_element(\"tag name\", \"body\").click()\n",
    "webElements = driver.find_element(\"tag name\", \"canvas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bff993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_rect() -> dict[str : Tuple[int, int]]:\n",
    "        game_rect : dict[str : Tuple[int, int]] = {}\n",
    "\n",
    "        browser_window = pyautogui.getWindowsWithTitle(\"Bad Time Simulator\")[0]  # or \"Chrome\" if that's what appears in the title\n",
    "        \n",
    "        # Get window coordinates\n",
    "        x, y = browser_window.left, browser_window.top\n",
    "        width, height = browser_window.width, browser_window.height\n",
    "        \n",
    "        # Adjust these values to capture just the game area (not the browser UI)\n",
    "        game_x = x + 10  # Add padding to skip browser borders\n",
    "        game_y = y + 100  # Skip address bar and tabs\n",
    "        game_width = width - 20\n",
    "        game_height = height - 120\n",
    "\n",
    "        game_rect[\"corrdinates\"] = [game_x, game_y]\n",
    "        game_rect[\"scale\"] = [game_width, game_height]\n",
    "\n",
    "        return game_rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b2bfe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_game_state():\n",
    "    player_y = get_player_y()\n",
    "    bones = get_bone_positions()\n",
    "    \n",
    "    # Create 3x3 grid (9 cells) to encode bone presence\n",
    "    grid = np.zeros(9)  # Represents the battle area\n",
    "    \n",
    "    for x, y in bones:\n",
    "        # Map bone position to grid cell (example mapping)\n",
    "        col = min(2, x // (400 // 3))  # 400 = region width\n",
    "        row = min(2, y // (300 // 3))  # 300 = region height\n",
    "        grid[row * 3 + col] = 1  # Mark cell as occupied\n",
    "    \n",
    "    # State: [player_y, grid_0, grid_1, ..., grid_8]\n",
    "    return np.concatenate([[player_y], grid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0b09489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_hit(state):\n",
    "    # Check if player's red heart pixel is missing (got hit)\n",
    "    return state[player_y, new_player_y] != RED_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de277d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(action):\n",
    "    body = driver.find_element(\"tag name\", \"body\")\n",
    "    if action == 0:    # Move left\n",
    "        body.send_keys(Keys.ARROW_LEFT)\n",
    "    elif action == 1:  # Move right\n",
    "        body.send_keys(Keys.ARROW_RIGHT)\n",
    "    elif action == 2:  # Move up\n",
    "        body.send_keys(Keys.ARROW_UP)\n",
    "    elif action == 3:  # Move down\n",
    "        body.send_keys(Keys.ARROW_DOWN)\n",
    "    elif action == 4:  # Attack (Z key)\n",
    "        body.send_keys(\"z\")\n",
    "    elif action == 5:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b544f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state, next_state):\n",
    "    player_y = state[0]\n",
    "    new_player_y = next_state[0]\n",
    "    bones = state[1:]  # Enemy bone positions\n",
    "    \n",
    "    # Penalize getting hit\n",
    "    if check_collision(player_y, bones):\n",
    "        return -100\n",
    "    \n",
    "    # Reward moving closer to safe spots\n",
    "    closest_bone = min(bones, key=lambda y: abs(y - player_y))\n",
    "    reward = -abs(player_y - closest_bone) * 0.1\n",
    "    \n",
    "    # Small reward for surviving\n",
    "    return reward + 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "302e7ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wreep\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument(s) not recognized: {'lr': 0.001}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m batch_size = \u001b[32m32\u001b[39m\n\u001b[32m      2\u001b[39m episodes = \u001b[32m1000\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m agent = \u001b[43mDQNAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust state_size based on bones\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[32m      6\u001b[39m     reset_game()  \u001b[38;5;66;03m# You'll need to implement this (e.g., press R key)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mDQNAgent.__init__\u001b[39m\u001b[34m(self, state_size, action_size)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mself\u001b[39m.epsilon_decay = \u001b[32m0.995\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mself\u001b[39m.learning_rate = \u001b[32m0.001\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mDQNAgent._build_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     16\u001b[39m model.add(Dense(\u001b[32m24\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     17\u001b[39m model.add(Dense(\u001b[38;5;28mself\u001b[39m.action_size, activation=\u001b[33m'\u001b[39m\u001b[33mlinear\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m model.compile(loss=\u001b[33m'\u001b[39m\u001b[33mmse\u001b[39m\u001b[33m'\u001b[39m, optimizer=\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wreep\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\adam.py:62\u001b[39m, in \u001b[36mAdam.__init__\u001b[39m\u001b[34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     45\u001b[39m     learning_rate=\u001b[32m0.001\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m     **kwargs,\n\u001b[32m     61\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclipnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclipvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclipvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m        \u001b[49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_ema\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_ema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43mema_momentum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mema_momentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_scale_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_scale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m.beta_1 = beta_1\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mself\u001b[39m.beta_2 = beta_2\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wreep\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py:21\u001b[39m, in \u001b[36mTFOptimizer.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mself\u001b[39m._distribution_strategy = tf.distribute.get_strategy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wreep\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:90\u001b[39m, in \u001b[36mBaseOptimizer.__init__\u001b[39m\u001b[34m(self, learning_rate, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m     warnings.warn(\n\u001b[32m     87\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mArgument `decay` is no longer supported and will be ignored.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     88\u001b[39m     )\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mArgument(s) not recognized: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     93\u001b[39m     name = auto_name(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Argument(s) not recognized: {'lr': 0.001}"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "episodes = 1000\n",
    "agent = DQNAgent(state_size = 10, action_size=6)  # Adjust state_size based on bones\n",
    "\n",
    "for e in range(episodes):\n",
    "    reset_game()  # You'll need to implement this (e.g., press R key)\n",
    "    state = get_game_state()\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        take_action(action)  # Press UP/DOWN keys\n",
    "        \n",
    "        next_state = get_game_state()\n",
    "        reward = get_reward(state, next_state)\n",
    "        done = check_game_over()  # Check if player died\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(f\"Episode: {e}, Reward: {total_reward}\")\n",
    "            break\n",
    "        \n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.train(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899866f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5be87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
