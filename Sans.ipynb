{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d810585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import Dense\n",
    "from keras.api.optimizers import Adam\n",
    "from collections import deque\n",
    "from IPython.display import IFrame\n",
    "import pyautogui\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "from PIL import ImageGrab\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf3b9738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_player_y():\n",
    "    screenshot = pyautogui.screenshot()\n",
    "    img = cv2.cvtColor(np.array(screenshot), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Detect red heart (adjust HSV thresholds)\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    lower_red = np.array([0, 120, 70])\n",
    "    upper_red = np.array([10, 255, 255])\n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    \n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        return y + h // 2  # Middle of the heart\n",
    "    return 0.0  # Fallback  \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1ae3aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bone_positions():\n",
    "    screenshot = pyautogui.screenshot()\n",
    "    img = cv2.cvtColor(np.array(screenshot), cv2.COLOR_RGB2GRAY)\n",
    "    _, threshold = cv2.threshold(img, 200, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(threshold, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    return [cv2.boundingRect(c)[1] for c in contours]  # List of bone y-positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15022cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size.\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0   # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state[np.newaxis, :])[0])\n",
    "            target_f = self.model.predict(state[np.newaxis, :])\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state[np.newaxis, :], target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "# You would need to modify the game to:\n",
    "# 1. Provide state observations (player position, bullet positions, etc.)\n",
    "# 2. Accept discrete actions (move left, right, up, down, etc.)\n",
    "# 3. Calculate rewards (survival time, damage dealt, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcdd75a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Launch Chrome and load the game\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://jcw87.github.io/c2-sans-fight/\")  # Hosted game URL\n",
    "time.sleep(2)  # Wait for game to load\n",
    "\n",
    "# Focus the game window\n",
    "driver.find_element(\"tag name\", \"body\").click()\n",
    "webElements = driver.find_element(\"tag name\", \"canvas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bff993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_rect() -> dict[str : Tuple[int, int]]:\n",
    "        game_rect : dict[str : Tuple[int, int]] = {}\n",
    "\n",
    "        browser_window = pyautogui.getWindowsWithTitle(\"Bad Time Simulator\")[0]  # or \"Chrome\" if that's what appears in the title\n",
    "        \n",
    "        # Get window coordinates\n",
    "        x, y = browser_window.left, browser_window.top\n",
    "        width, height = browser_window.width, browser_window.height\n",
    "        \n",
    "        # Adjust these values to capture just the game area (not the browser UI)\n",
    "        game_x = x + 10  # Add padding to skip browser borders\n",
    "        game_y = y + 100  # Skip address bar and tabs\n",
    "        game_width = width - 20\n",
    "        game_height = height - 120\n",
    "\n",
    "        game_rect[\"corrdinates\"] = [game_x, game_y]\n",
    "        game_rect[\"scale\"] = [game_width, game_height]\n",
    "\n",
    "        return game_rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b2bfe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_game_state():\n",
    "    player_y = get_player_y()  # From Step 1\n",
    "    bones = get_bone_positions()  # List of bone y-positions (you'll need to implement this)\n",
    "    state = np.array([player_y] + bones)\n",
    "    return np.expand_dims(state, axis=-1)  # Add channel dim for CNN (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0b09489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_hit(state):\n",
    "    # Check if player's red heart pixel is missing (got hit)\n",
    "    return state[player_y, new_player_y] != RED_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de277d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(action):\n",
    "    body = driver.find_element(\"tag name\", \"body\")\n",
    "    if action == 0:    # Move left\n",
    "        body.send_keys(Keys.ARROW_LEFT)\n",
    "    elif action == 1:  # Move right\n",
    "        body.send_keys(Keys.ARROW_RIGHT)\n",
    "    elif action == 2:  # Move up\n",
    "        body.send_keys(Keys.ARROW_UP)\n",
    "    elif action == 3:  # Move down\n",
    "        body.send_keys(Keys.ARROW_DOWN)\n",
    "    elif action == 4:  # Attack (Z key)\n",
    "        body.send_keys(\"z\")\n",
    "    elif action == 5:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b544f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state, next_state):\n",
    "    player_y = state[0]\n",
    "    new_player_y = next_state[0]\n",
    "    bones = state[1:]  # Enemy bone positions\n",
    "    \n",
    "    # Penalize getting hit\n",
    "    if check_collision(player_y, bones):\n",
    "        return -100\n",
    "    \n",
    "    # Reward moving closer to safe spots\n",
    "    closest_bone = min(bones, key=lambda y: abs(y - player_y))\n",
    "    reward = -abs(player_y - closest_bone) * 0.1\n",
    "    \n",
    "    # Small reward for surviving\n",
    "    return reward + 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e7ccd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid dtype: function",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m batch_size = \u001b[32m32\u001b[39m\n\u001b[32m      2\u001b[39m episodes = \u001b[32m1000\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m agent = \u001b[43mDQNAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_bone_positions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust state_size based on bones\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[32m      6\u001b[39m     reset_game()  \u001b[38;5;66;03m# You'll need to implement this (e.g., press R key)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mDQNAgent.__init__\u001b[39m\u001b[34m(self, state_size, action_size)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mself\u001b[39m.epsilon_decay = \u001b[32m0.995\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mself\u001b[39m.learning_rate = \u001b[32m0.001\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mDQNAgent._build_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_build_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     14\u001b[39m     model = Sequential()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrelu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     model.add(Dense(\u001b[32m24\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     17\u001b[39m     model.add(Dense(\u001b[38;5;28mself\u001b[39m.action_size, activation=\u001b[33m'\u001b[39m\u001b[33mlinear\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wreep\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\sequential.py:88\u001b[39m, in \u001b[36mSequential.add\u001b[39m\u001b[34m(self, layer, rebuild)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._layers:\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(layer, \u001b[33m\"\u001b[39m\u001b[33m_input_shape_arg\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m         \u001b[38;5;28mself\u001b[39m.add(\u001b[43mInputLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_input_shape_arg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# If we are passed a Keras tensor created by keras.Input(), we\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# extract the input layer from its keras history and use that.\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(layer, \u001b[33m\"\u001b[39m\u001b[33m_keras_history\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wreep\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:92\u001b[39m, in \u001b[36mInputLayer.__init__\u001b[39m\u001b[34m(self, shape, batch_size, dtype, sparse, ragged, batch_shape, input_tensor, optional, name, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou must pass a `shape` argument.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m         shape = \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstandardize_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m         batch_shape = (batch_size,) + shape\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m._batch_shape = backend.standardize_shape(batch_shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wreep\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py:581\u001b[39m, in \u001b[36mstandardize_shape\u001b[39m\u001b[34m(shape)\u001b[39m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.backend() == \u001b[33m\"\u001b[39m\u001b[33mjax\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_DimExpr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(e)):\n\u001b[32m    579\u001b[39m     \u001b[38;5;66;03m# JAX2TF tracing uses JAX-native dimension expressions\u001b[39;00m\n\u001b[32m    580\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_int_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    582\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    583\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot convert \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m to a shape. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    584\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound invalid entry \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m of type \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    585\u001b[39m     )\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m e < \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wreep\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py:612\u001b[39m, in \u001b[36mis_int_dtype\u001b[39m\u001b[34m(dtype)\u001b[39m\n\u001b[32m    610\u001b[39m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mkeras.backend.is_int_dtype\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_int_dtype\u001b[39m(dtype):\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     dtype = \u001b[43mstandardize_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dtype.startswith(\u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m dtype.startswith(\u001b[33m\"\u001b[39m\u001b[33muint\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wreep\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py:553\u001b[39m, in \u001b[36mstandardize_dtype\u001b[39m\u001b[34m(dtype)\u001b[39m\n\u001b[32m    550\u001b[39m     dtype = dtype.\u001b[34m__name__\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dtypes.ALLOWED_DTYPES:\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dtype\n",
      "\u001b[31mValueError\u001b[39m: Invalid dtype: function"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "episodes = 1000\n",
    "agent = DQNAgent(state_size = get_bone_positions(), action_size=6)  # Adjust state_size based on bones\n",
    "\n",
    "for e in range(episodes):\n",
    "    reset_game()  # You'll need to implement this (e.g., press R key)\n",
    "    state = get_game_state()\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        take_action(action)  # Press UP/DOWN keys\n",
    "        \n",
    "        next_state = get_game_state()\n",
    "        reward = get_reward(state, next_state)\n",
    "        done = check_game_over()  # Check if player died\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(f\"Episode: {e}, Reward: {total_reward}\")\n",
    "            break\n",
    "        \n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.train(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899866f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5be87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
